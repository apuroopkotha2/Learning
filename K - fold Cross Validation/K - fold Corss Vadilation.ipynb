{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae674ba",
   "metadata": {},
   "source": [
    "# Assignment : - \n",
    "\n",
    "To Learn K - fold Cross Validation for model evaluation and follow the link - https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/ and try to improve the accuracy. \n",
    "\n",
    "Reference: - \n",
    "Brownlee. (2018, May 23). A Gentle Introduction to k-fold Cross-Validation. Machine Learning Mastery. https://machinelearningmastery.com/k-fold-cross-validation/. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fe538",
   "metadata": {},
   "source": [
    "### K - fold cross - validation\n",
    "K-fold cross-validation is a widely used technique in machine learning for evaluating the performance of a model. It helps to estimate the model's accuracy and generalization ability by partitioning the available dataset into multiple subsets or folds. This approach provides a more reliable assessment of the model's performance compared to a single train-test split.\n",
    "\n",
    "Here's an overview of how k-fold cross-validation works:\n",
    "\n",
    "1. **Partitioning the Data**: The original dataset is divided into k equal-sized folds. Each fold is a separate subset of the data that will be used as a validation set.\n",
    "\n",
    "2. **Training and Evaluation**: The model is trained k times, each time using k-1 folds as the training set and one fold as the validation set. In each iteration, the model is trained on a different combination of folds.\n",
    "\n",
    "3. **Performance Evaluation**: After each training iteration, the model's performance is evaluated using the validation set (the fold that was not used for training). The evaluation metric (e.g., accuracy, F1-score) is computed and recorded.\n",
    "\n",
    "4. **Aggregating Results**: The performance metrics obtained in each iteration are averaged to obtain a single estimate of the model's performance. This average performance metric is considered as the overall performance of the model.\n",
    "\n",
    "Key advantages of k-fold cross-validation:\n",
    "\n",
    "1. **Better Model Assessment**: By evaluating the model on multiple different validation sets, k-fold cross-validation provides a more comprehensive assessment of the model's performance. It reduces the impact of the specific data split on the evaluation results.\n",
    "\n",
    "2. **Efficient Use of Data**: K-fold cross-validation makes efficient use of the available data. It uses a large portion of the dataset for training the model, and each data point is used for both training and validation.\n",
    "\n",
    "3. **Parameter Tuning and Model Selection**: K-fold cross-validation is commonly used for hyperparameter tuning and model selection. It allows comparing different models or tuning hyperparameters by assessing their performance across multiple folds.\n",
    "\n",
    "4. **More Reliable Estimate**: The averaged performance metric obtained from k-fold cross-validation tends to be a more reliable estimate of the model's performance compared to a single train-test split. It provides a more stable and less biased evaluation.\n",
    "\n",
    "There are variations of k-fold cross-validation, such as stratified k-fold (preserves the class distribution in each fold) and repeated k-fold (repeats the process multiple times with different random splits). These variations can be used depending on the specific requirements of the problem.\n",
    "\n",
    "K-fold cross-validation is a valuable tool for model evaluation and selection, helping to assess the model's performance and estimate its ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154cc98",
   "metadata": {},
   "source": [
    "The below is the codes provided in the provided link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38c13dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f6e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868 (0.032)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211726fd",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31879",
   "metadata": {},
   "source": [
    "To improve the accuracy of the logistic regression model, the following approaches can be implemented:\n",
    "\n",
    "Feature Selection: Perform feature selection techniques to identify the most informative features. This can help in reducing noise and improving the model's ability to generalize. You can use methods like recursive feature elimination (RFE), L1 regularization (Lasso), or information gain to select the most relevant features.\n",
    "\n",
    "Feature Engineering: Create new features that might be more informative for the classification task and try combining existing features, creating interaction terms, or transforming variables to capture non-linear relationships.\n",
    "\n",
    "Algorithm Selection: Logistic regression may not always be the best algorithm for a given dataset. but can be tried on other classification algorithms such as random forest, support vector machines (SVM), gradient boosting, or neural networks to see if they provide better accuracy.\n",
    "\n",
    "Hyperparameter Tuning: Optimize the hyperparameters of the logistic regression model to find the best combination for the dataset, can use techniques like grid search or randomized search to explore different parameter values and evaluate their impact on model performance.\n",
    "\n",
    "Ensemble Methods: Utilize ensemble methods to combine multiple models and improve accuracy. For example, we can use techniques like bagging (e.g., random forest) or boosting (e.g., AdaBoost, XGBoost) to create an ensemble of models that work together to make predictions.\n",
    "\n",
    "Data Preprocessing: Ensure that the data is properly preprocessed before training the model. This includes handling missing values, scaling numerical features, and encoding categorical variables appropriately.\n",
    "\n",
    "Collect More Data: If possible, collecting more data can help improve the model's accuracy. A larger dataset can provide more diverse samples, reducing overfitting and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26410a",
   "metadata": {},
   "source": [
    "### Model 1: - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adb650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865 (0.031)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV  # Use LogisticRegressionCV for automatic regularization\n",
    "from sklearn.preprocessing import StandardScaler  # Standardize the input features\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# create model with automatic regularization (LogisticRegressionCV)\n",
    "model = LogisticRegressionCV(cv=cv, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba393cc",
   "metadata": {},
   "source": [
    "### Model 2 :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e296840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.876\n",
      "Best Parameters: {'randomforestclassifier__max_depth': None, 'randomforestclassifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Define the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Create a pipeline with a Random Forest classifier\n",
    "pipeline = make_pipeline(RandomForestClassifier(random_state=1))\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "    'randomforestclassifier__max_depth': [None, 5, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Report performance\n",
    "print('Best Accuracy: %.3f' % grid_search.best_score_)\n",
    "print('Best Parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff8ad31",
   "metadata": {},
   "source": [
    "Explanation:- \n",
    "In this code, the following changes have been made:\n",
    "\n",
    "Feature Selection: The code now includes feature selection using the SelectKBest class from scikit-learn. It selects the top k features based on the ANOVA F-value between the feature and the target, we can adjust the value of k according to the dataset.\n",
    "\n",
    "Data Preprocessing: The code incorporates standardization using the StandardScaler class to scale the selected features. This is important to ensure that all features are on a similar scale, which can improve the performance of the logistic regression model.\n",
    "\n",
    "Algorithm Selection: The logistic regression model has been replaced with a Random Forest classifier from the RandomForestClassifier class. Random Forests can often provide better accuracy by capturing non-linear relationships in the data and handling feature interactions.\n",
    "\n",
    "Hyperparameter Tuning: The code now performs hyperparameter tuning using grid search with cross-validation (GridSearchCV). It searches over different combinations of the n_estimators (number of trees) and max_depth (maximum depth of each tree) hyperparameters for the Random Forest classifier.\n",
    "\n",
    "By incorporating these techniques, we can explore different feature subsets, optimize hyperparameters, and utilize a more powerful algorithm to potentially achieve better accuracy on the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220619cb",
   "metadata": {},
   "source": [
    "### Model 3 :- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de850a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.881\n",
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Define the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "model = XGBClassifier(random_state=1)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Report performance\n",
    "print('Best Accuracy: %.3f' % grid_search.best_score_)\n",
    "print('Best Parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f918b6",
   "metadata": {},
   "source": [
    "Explanation: - \n",
    "Here, I wanted to achieve accuracy above 90%, So i have tried a model using a more advanced algorithm such as a Gradient Boosting Classifier (e.g., XGBoost, LightGBM). These algorithms are known for their strong predictive performance. \n",
    "\n",
    "In this updated code, the changes include:\n",
    "\n",
    "Algorithm Selection: The code now uses the XGBoost algorithm (XGBClassifier) instead of logistic regression or random forest. XGBoost is a gradient boosting algorithm known for its strong predictive performance.\n",
    "\n",
    "Hyperparameter Tuning: The code performs hyperparameter tuning using grid search with cross-validation (GridSearchCV) to find the best combination of hyperparameters for the XGBoost classifier. The parameters being tuned include n_estimators (number of trees in the ensemble), max_depth (maximum depth of each tree), and learning_rate (shrinkage factor for each tree).\n",
    "\n",
    "By incorporating XGBoost and optimizing its hyperparameters, I thought there might be a higher chance of achieving accuracy above 90%. However, the success of the model depends on the specific dataset and problem at hand. It's important to experiment with different algorithms, feature selection, and hyperparameter settings to find the best solution for the given classification task. And after impementing the XGBoost I have a bit higher accuracy when compared to all the above of 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340655b5",
   "metadata": {},
   "source": [
    "## Repeated K - Fold Cross - Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28eae57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code:- \n",
    "...\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68934bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867 (0.031)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate a logistic regression model using repeated k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# prepare the cross-validation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36d5601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 mean=0.8680 se=0.011\n",
      ">2 mean=0.8675 se=0.008\n",
      ">3 mean=0.8673 se=0.006\n",
      ">4 mean=0.8670 se=0.006\n",
      ">5 mean=0.8658 se=0.005\n",
      ">6 mean=0.8655 se=0.004\n",
      ">7 mean=0.8651 se=0.004\n",
      ">8 mean=0.8651 se=0.004\n",
      ">9 mean=0.8656 se=0.003\n",
      ">10 mean=0.8658 se=0.003\n",
      ">11 mean=0.8655 se=0.003\n",
      ">12 mean=0.8654 se=0.003\n",
      ">13 mean=0.8652 se=0.003\n",
      ">14 mean=0.8651 se=0.003\n",
      ">15 mean=0.8653 se=0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# compare the number of repeats for repeated k-fold cross-validation\n",
    "from scipy.stats import sem\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, repeats):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "\t# create model\n",
    "\tmodel = LogisticRegression()\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# configurations to test\n",
    "repeats = range(1,16)\n",
    "results = list()\n",
    "for r in repeats:\n",
    "\t# evaluate using a given number of repeats\n",
    "\tscores = evaluate_model(X, y, r)\n",
    "\t# summarize\n",
    "\tprint('>%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))\n",
    "\t# store\n",
    "\tresults.append(scores)\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f4f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.921 (0.028)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# feature engineering - perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# create model pipeline with random forest classifier\n",
    "model = make_pipeline(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=1)\n",
    ")\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X_scaled, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bdba4",
   "metadata": {},
   "source": [
    "In this enhanced code I have implemented the following:\n",
    "\n",
    "Feature Engineering: The code incorporates feature scaling using StandardScaler to standardize the input features (X). This step helps in bringing all features to a similar scale and can improve the performance of some models.\n",
    "\n",
    "Algorithm Selection: The code uses a RandomForestClassifier as the classification algorithm instead of logistic regression. Random forest is an ensemble method that combines multiple decision trees and can handle complex relationships in the data.\n",
    "\n",
    "Hyperparameter Tuning: The code sets the number of estimators in the random forest classifier to 100. we can experiment with different values or perform hyperparameter tuning using techniques like grid search or random search to find the optimal value.\n",
    "\n",
    "Data Preprocessing: The feature scaling using StandardScaler is applied to the input features (X) before feeding them into the model. This step ensures that all features have zero mean and unit variance.\n",
    "\n",
    "By incorporating these changes, the code aims to improve the accuracy by using a random forest classifier, feature scaling, and standardized input features. It's essential to experiment with different approaches and techniques to find the best solution for your classification task just by performing Trail and Error methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e507127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.929 (0.023)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# feature engineering - perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# create model pipeline with XGBoost classifier\n",
    "model = make_pipeline(\n",
    "    XGBClassifier(n_estimators=100, random_state=1)\n",
    ")\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X_scaled, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1782143",
   "metadata": {},
   "source": [
    "In this code, the key changes are:\n",
    "\n",
    "Algorithm Selection: The code uses the XGBClassifier from the XGBoost library as the classification algorithm. XGBoost is a powerful gradient boosting algorithm known for its performance and ability to handle complex datasets.\n",
    "\n",
    "Hyperparameter Tuning: The number of estimators in the XGBoost classifier is set to 100, similar to the random forest classifier. we can experiment with different hyperparameters like learning rate, maximum depth, or subsample ratio to further improve the accuracy.\n",
    "\n",
    "By incorporating these changes, the code now uses XGBoost as an alternative model to test for better accuracy. \n",
    "By this code we can observe that the accuracy has been improved from 0.867 initially to 0.921 when RandomForestClassifier used and to 0.929 when XGboost has been implemented. This might not be a higehr increase in the accuracy but this gives us an idea that different models can be used to study the performance and the accuracy on different datasets and aims. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d72880",
   "metadata": {},
   "source": [
    "#### Why I have used XGBoost and Random Forest Classifiers in this noetebook: - \n",
    "\n",
    "XGBoost and Random Forest classifiers are commonly used in classification problems for several reasons:\n",
    "\n",
    "1. **Ensemble Methods**: Both XGBoost and Random Forest are ensemble methods that combine multiple individual classifiers (decision trees) to make final predictions. Ensemble methods often outperform individual models by reducing bias, variance, and overfitting.\n",
    "\n",
    "2. **Handling Nonlinear Relationships**: XGBoost and Random Forest can capture complex nonlinear relationships between features and the target variable. They are capable of learning intricate decision boundaries and can handle high-dimensional data effectively.\n",
    "\n",
    "3. **Robustness to Outliers and Irrelevant Features**: Ensemble methods like XGBoost and Random Forest are generally robust to outliers and irrelevant features in the dataset. They are less prone to overfitting on noisy or irrelevant data, which can lead to better generalization and improved accuracy.\n",
    "\n",
    "4. **Feature Importance**: Both XGBoost and Random Forest provide feature importance measures. These measures help identify the most informative features in the dataset, allowing for better understanding of the underlying relationships and potential feature selection.\n",
    "\n",
    "5. **Parallelization and Efficiency**: XGBoost and Random Forest algorithms are designed to take advantage of parallel processing capabilities. They can be easily parallelized, allowing for faster training and prediction times, especially for large datasets.\n",
    "\n",
    "###### Note: - \n",
    "The choice of algorithm depends on the specific problem, dataset characteristics, and performance requirements. Other algorithms such as Logistic Regression, Support Vector Machines, Neural Networks, or even simpler models like Naive Bayes can also be effective depending on the nature of the problem and the data. It's always a good practice to experiment with different algorithms and evaluate their performance to choose the best model for a given problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
